<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Big-O</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to your external CSS file -->
    <!-- Load MathJax and configure for proper display of math -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script>
    MathJax = {
        tex: {
            inlineMath: [['\\(', '\\)']],
            displayMath: [['\\[', '\\]']]
        },
        svg: {
            fontCache: 'global'
        }
    };
    </script>
</head>
<body>
    <div class="content">
        <h1>Introduction and Big-O</h1>
        
        <h2>Algorithms</h2>
        <p>
            An <strong>algorithm</strong> is a step-by-step procedure or formula for solving a problem. Typically, an algorithm is a set of well-defined instructions to perform a computation or to process data. Algorithms are fundamental to all aspects of computer science and are used in software development, data processing, artificial intelligence, and more.
        </p>
        <p>
            The <strong>efficiency</strong> of an algorithm is crucial because it directly impacts the performance and scalability of applications. Efficient algorithms can handle larger data sets and more complex calculations within reasonable time frames and using acceptable amounts of resources. Efficiency in algorithms is typically measured in terms of:
            <ul>
                <li><strong>Time Complexity:</strong> How the running time of an algorithm increases as the size of the input data grows. It is often expressed using Big-O notation, which abstracts away constants and lower-order terms to focus on the most significant factors that affect growth rate.</li>
                <li><strong>Space Complexity:</strong> How much memory an algorithm uses in relation to the input size. Like time complexity, it helps determine the scalability of an algorithm as the size of input data increases.</li>
            </ul>
            Improving algorithm efficiency can reduce processing times and resource consumption, leading to better user experiences and lower operational costs, particularly in data-intensive and critical applications.
        </p>

        <h2>Big-O</h2>
        <p>
            Simplifying Big-O notation involves two key principles:
            <ol>
                <li><strong>Ignoring constant factors:</strong> Constants do not affect the growth rate from an asymptotic perspective. For example, \\( O(2n) \\) simplifies to \\( O(n) \\) because the constant factor '2' does not change the linear nature of the growth rate.</li>
                <li><strong>Focusing on the dominant term:</strong> When multiple terms are involved, only the term with the largest growth rate is considered since it will dominate the others as the input size grows. For instance, \\( O(n^2 + n) \\) simplifies to \\( O(n^2) \\) because \\( n^2 \\) grows faster than \\( n \\) and will be the dominant term for large values of \\( n \\).</li>
            </ol>
        </p>
        <p>
            Big-O notation and "worst case" refer to different concepts in algorithm analysis. <strong>Big-O</strong> is a mathematical notation describing the upper bound of a function's growth rate, applicable to any scenarioâ€”best, average, or worst case. It focuses on how the time or space complexity scales with input size but doesn't specify actual performance metrics. <strong>Worst case</strong>, on the other hand, specifically refers to the maximum time or space an algorithm will require for any input of a given size. Thus, while Big-O can describe the complexity of a worst-case scenario, it is not limited to it and does not measure actual execution time or resource usage.
        </p>   
        <h2>Big-O, Big-Omega, Big-Theta</h2>     
        <p>
            <span>\\( f(n) = O(g(n)) \\)</span> if and only if there exists constants <span>\\( c > 0 \\)</span> and <span>\\( n_0 > 0 \\)</span> such that <span>\\( 0 \leq f(n) \leq c \cdot g(n) \\)</span> for all <span>\\( n \geq n_0 \\)</span>.
        </p>
        <p>
            <span>\\( f(n) = \Omega(g(n)) \\)</span> if and only if there exists constants <span>\\( c > 0 \\)</span> and <span>\\( n_0 > 0 \\)</span> such that <span>\\( 0 \leq c \cdot g(n) \leq f(n) \\)</span> for all <span>\\( n \geq n_0 \\)</span>.
        </p>
        <p>
            <span>\\( f(n) = \Theta(g(n)) \\)</span> if and only if there exists constants <span>\\( c_1 > 0 \\)</span>, <span>\\( c_2 > 0 \\)</span> and <span>\\( n_0 > 0 \\)</span> such that <span>\\( 0 \leq c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) \\)</span> for all <span>\\( n \geq n_0 \\)</span>.
        </p>
        
        <h2>First Order Linear Recurrences</h2>
        <p>The first-order linear recurrence relation describes a sequence where each term is defined based on the preceding term and possibly some constant or coefficient. A common example is the arithmetic sequence.</p>
        <p>
            A first-order linear recurrence relation can be expressed as:
            <span>\\( a_{n} = r \cdot a_{n-1} + d \\)</span>,
            where \\( r \\) and \\( d \\) are constants, and \\( a_{n} \\) represents the \\( n^{th} \\) term of the sequence.
        </p>
        <p>
            The sum of the first \\( n \\) integers can be expressed using the summation notation:
            <span>\\( S = \sum_{i=1}^{n} i = \frac{n(n+1)}{2} \\)</span>.
        </p>
        
        <h2>Master Theorem</h2>
        <p>
            The Master Theorem helps in analyzing the time complexity of recurrence relations of the form:
            <span>\\( T(n) = aT(\frac{n}{b}) + f(n) \\)</span>, where:
            <ul>
                <li>\\( a \geq 1 \\)</li>
                <li>\\( b > 1 \\)</li>
                <li>\\( f(n) \\) is asymptotically positive.</li>
            </ul>
            It concludes that:
            <ul>
                <li>If \\( f(n) = O(n^{\log_b{a-\epsilon}}) \\) for some \\( \epsilon > 0 \\), then \\( T(n) = \Theta(n^{\log_b{a}}) \\).</li>
                <li>If \\( f(n) = \Theta(n^{\log_b{a}} \log^k n) \\) for some \\( k \geq 0 \\), then \\( T(n) = \Theta(n^{\log_b{a}} \log^{k+1} n) \\).</li>
                <li>If \\( f(n) = \Omega(n^{\log_b{a+\epsilon}}) \\) for some \\( \epsilon > 0 \\), and if \\( af(n/b) \leq cf(n) \\) for some \\( c < 1 \\) and sufficiently large \\( n \\), then \\( T(n) = \Theta(f(n)) \\).</li>
            </ul>
        </p>

        <h2>Binary Search</h2>
        <p>
            Binary search is an efficient algorithm for finding the position of an element in a sorted array. The algorithm compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated, and the search continues on the remaining half, again taking the middle element to compare with the target. This process can be represented by the following recurrence relation:
            <span>\\( T(n) = T(\frac{n}{2}) + O(1) \\)</span>.
        </p>        

    </div>
</body>
</html>
